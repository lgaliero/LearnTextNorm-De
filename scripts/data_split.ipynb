{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37820a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV from: all_corpora.csv\n",
      "\n",
      "Total sentences loaded: 27889\n",
      "Columns: ['corpus', 'lang_prof', 'xml_file', 'file_id', 'sent_num', 'src', 'tgt', 'corrected']\n",
      "\n",
      "============================================================\n",
      "PERFORMING STRATIFIED K-FOLD SPLIT (using sklearn)\n",
      "============================================================\n",
      "\n",
      "=== FOLD 1/5 ===\n",
      "Train size: 22311, Test size: 5578\n",
      "\n",
      "Train set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 4013 (17.99%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 4065 (18.22%) [target: 27.28%]\n",
      "    Kolipsi_2: 10714 (48.02%) [target: 30.76%]\n",
      "    LEONIDE: 3519 (15.77%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 14583 (65.36%) [target: 65.00%]\n",
      "    corrected: 7728 (34.64%) [target: 35.00%]\n",
      "\n",
      "Test set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 1004 (18.00%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 1016 (18.21%) [target: 27.28%]\n",
      "    Kolipsi_2: 2678 (48.01%) [target: 30.76%]\n",
      "    LEONIDE: 880 (15.78%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 3646 (65.36%) [target: 65.00%]\n",
      "    corrected: 1932 (34.64%) [target: 35.00%]\n",
      "\n",
      "=== FOLD 2/5 ===\n",
      "Train size: 22311, Test size: 5578\n",
      "\n",
      "Train set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 4013 (17.99%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 4065 (18.22%) [target: 27.28%]\n",
      "    Kolipsi_2: 10714 (48.02%) [target: 30.76%]\n",
      "    LEONIDE: 3519 (15.77%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 14583 (65.36%) [target: 65.00%]\n",
      "    corrected: 7728 (34.64%) [target: 35.00%]\n",
      "\n",
      "Test set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 1004 (18.00%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 1016 (18.21%) [target: 27.28%]\n",
      "    Kolipsi_2: 2678 (48.01%) [target: 30.76%]\n",
      "    LEONIDE: 880 (15.78%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 3646 (65.36%) [target: 65.00%]\n",
      "    corrected: 1932 (34.64%) [target: 35.00%]\n",
      "\n",
      "=== FOLD 3/5 ===\n",
      "Train size: 22311, Test size: 5578\n",
      "\n",
      "Train set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 4014 (17.99%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 4065 (18.22%) [target: 27.28%]\n",
      "    Kolipsi_2: 10713 (48.02%) [target: 30.76%]\n",
      "    LEONIDE: 3519 (15.77%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 14583 (65.36%) [target: 65.00%]\n",
      "    corrected: 7728 (34.64%) [target: 35.00%]\n",
      "\n",
      "Test set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 1003 (17.98%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 1016 (18.21%) [target: 27.28%]\n",
      "    Kolipsi_2: 2679 (48.03%) [target: 30.76%]\n",
      "    LEONIDE: 880 (15.78%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 3646 (65.36%) [target: 65.00%]\n",
      "    corrected: 1932 (34.64%) [target: 35.00%]\n",
      "\n",
      "=== FOLD 4/5 ===\n",
      "Train size: 22311, Test size: 5578\n",
      "\n",
      "Train set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 4014 (17.99%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 4065 (18.22%) [target: 27.28%]\n",
      "    Kolipsi_2: 10713 (48.02%) [target: 30.76%]\n",
      "    LEONIDE: 3519 (15.77%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 14584 (65.37%) [target: 65.00%]\n",
      "    corrected: 7727 (34.63%) [target: 35.00%]\n",
      "\n",
      "Test set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 1003 (17.98%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 1016 (18.21%) [target: 27.28%]\n",
      "    Kolipsi_2: 2679 (48.03%) [target: 30.76%]\n",
      "    LEONIDE: 880 (15.78%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 3645 (65.35%) [target: 65.00%]\n",
      "    corrected: 1933 (34.65%) [target: 35.00%]\n",
      "\n",
      "=== FOLD 5/5 ===\n",
      "Train size: 22312, Test size: 5577\n",
      "\n",
      "Train set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 4014 (17.99%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 4064 (18.21%) [target: 27.28%]\n",
      "    Kolipsi_2: 10714 (48.02%) [target: 30.76%]\n",
      "    LEONIDE: 3520 (15.78%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 14583 (65.36%) [target: 65.00%]\n",
      "    corrected: 7729 (34.64%) [target: 35.00%]\n",
      "\n",
      "Test set:\n",
      "  Corpus distribution:\n",
      "    Kolipsi_1_L1: 1003 (17.98%) [target: 22.87%]\n",
      "    Kolipsi_1_L2: 1017 (18.24%) [target: 27.28%]\n",
      "    Kolipsi_2: 2678 (48.02%) [target: 30.76%]\n",
      "    LEONIDE: 879 (15.76%) [target: 19.08%]\n",
      "  Correction distribution:\n",
      "    left-as-is: 3646 (65.38%) [target: 65.00%]\n",
      "    corrected: 1931 (34.62%) [target: 35.00%]\n",
      "\n",
      "============================================================\n",
      "CREATING ALIGNMENT FILES\n",
      "============================================================\n",
      "\n",
      "Creating alignment files for: train\n",
      "Total sentence pairs: 22311\n",
      "✓ Alignment files created in: aligned_sets/fold_1\n",
      "  - train_src.txt / train_tgt.txt (plain)\n",
      "  - train_src.tok / train_tgt.tok (tokenized)\n",
      "  - train_src.segm / train_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: test\n",
      "Total sentence pairs: 5578\n",
      "✓ Alignment files created in: aligned_sets/fold_1\n",
      "  - test_src.txt / test_tgt.txt (plain)\n",
      "  - test_src.tok / test_tgt.tok (tokenized)\n",
      "  - test_src.segm / test_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: train\n",
      "Total sentence pairs: 22311\n",
      "✓ Alignment files created in: aligned_sets/fold_2\n",
      "  - train_src.txt / train_tgt.txt (plain)\n",
      "  - train_src.tok / train_tgt.tok (tokenized)\n",
      "  - train_src.segm / train_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: test\n",
      "Total sentence pairs: 5578\n",
      "✓ Alignment files created in: aligned_sets/fold_2\n",
      "  - test_src.txt / test_tgt.txt (plain)\n",
      "  - test_src.tok / test_tgt.tok (tokenized)\n",
      "  - test_src.segm / test_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: train\n",
      "Total sentence pairs: 22311\n",
      "✓ Alignment files created in: aligned_sets/fold_3\n",
      "  - train_src.txt / train_tgt.txt (plain)\n",
      "  - train_src.tok / train_tgt.tok (tokenized)\n",
      "  - train_src.segm / train_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: test\n",
      "Total sentence pairs: 5578\n",
      "✓ Alignment files created in: aligned_sets/fold_3\n",
      "  - test_src.txt / test_tgt.txt (plain)\n",
      "  - test_src.tok / test_tgt.tok (tokenized)\n",
      "  - test_src.segm / test_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: train\n",
      "Total sentence pairs: 22311\n",
      "✓ Alignment files created in: aligned_sets/fold_4\n",
      "  - train_src.txt / train_tgt.txt (plain)\n",
      "  - train_src.tok / train_tgt.tok (tokenized)\n",
      "  - train_src.segm / train_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: test\n",
      "Total sentence pairs: 5578\n",
      "✓ Alignment files created in: aligned_sets/fold_4\n",
      "  - test_src.txt / test_tgt.txt (plain)\n",
      "  - test_src.tok / test_tgt.tok (tokenized)\n",
      "  - test_src.segm / test_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: train\n",
      "Total sentence pairs: 22312\n",
      "✓ Alignment files created in: aligned_sets/fold_5\n",
      "  - train_src.txt / train_tgt.txt (plain)\n",
      "  - train_src.tok / train_tgt.tok (tokenized)\n",
      "  - train_src.segm / train_tgt.segm (segmented)\n",
      "\n",
      "Creating alignment files for: test\n",
      "Total sentence pairs: 5577\n",
      "✓ Alignment files created in: aligned_sets/fold_5\n",
      "  - test_src.txt / test_tgt.txt (plain)\n",
      "  - test_src.tok / test_tgt.tok (tokenized)\n",
      "  - test_src.segm / test_tgt.segm (segmented)\n",
      "\n",
      "============================================================\n",
      "✓ ALL PROCESSING COMPLETE\n",
      "============================================================\n",
      "Output directory: aligned_sets\n",
      "Number of folds: 5\n",
      "Method: sklearn.model_selection.StratifiedKFold\n",
      "\n",
      "You can access the splits programmatically:\n",
      "  splits[0] = (train_df_fold1, test_df_fold1)\n",
      "  splits[1] = (train_df_fold2, test_df_fold2)\n",
      "  etc.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Merged alignment preparation script with stratified K-fold cross-validation using sklearn\n",
    "Reads CSV, applies stratified sampling, and creates alignment input files\n",
    "\"\"\"\n",
    "#1 FIXED tested sample randomly and in equal proportion\n",
    "#2 stratified library random\n",
    "#3 FIXED sets - not Cross-Val\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ============================================================\n",
    "# TOKENIZATION\n",
    "# ============================================================\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Fast regex tokenizer\"\"\"\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def segment_with_underscores(text):\n",
    "    \"\"\"Segment text with underscores between tokens\"\"\"\n",
    "    return \"_\".join(simple_tokenize(text))\n",
    "\n",
    "# ============================================================\n",
    "# STRATIFIED SAMPLING WITH SKLEARN\n",
    "# ============================================================\n",
    "\n",
    "def stratified_split(df, k_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform stratified K-fold split using sklearn.model_selection.StratifiedKFold\n",
    "    Maintains:\n",
    "    - 65% left-as-is (corrected=False) / 35% corrected (corrected=True)\n",
    "    - Same corpus proportions in each fold\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with columns 'corpus', 'corrected', 'src', 'tgt'\n",
    "    k_folds : number of folds for cross-validation\n",
    "    random_state : random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples: [(train_df, test_df), ...] for each fold\n",
    "    \"\"\"\n",
    "    # Target proportions for reference\n",
    "    target_corpus_props = {\n",
    "        'Kolipsi_1_L1': 0.2287,\n",
    "        'Kolipsi_1_L2': 0.2728,\n",
    "        'Kolipsi_2': 0.3076,\n",
    "        'LEONIDE': 0.1908\n",
    "    }\n",
    "    \n",
    "    target_correction_props = {\n",
    "        False: 0.65,  # left-as-is\n",
    "        True: 0.35    # corrected\n",
    "    }\n",
    "    \n",
    "    # Create stratification key combining corpus and correction status\n",
    "    df['strat_key'] = df['corpus'].astype(str) + '_' + df['corrected'].astype(str)\n",
    "    \n",
    "    # Initialize StratifiedKFold from sklearn\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Get indices for X (features) and y (stratification labels)\n",
    "    X = df.index.to_numpy()\n",
    "    y = df['strat_key'].to_numpy()\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    # Generate train/test splits using sklearn\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(skf.split(X, y), 1):\n",
    "        train_df = df.iloc[train_indices].copy()\n",
    "        test_df = df.iloc[test_indices].copy()\n",
    "        \n",
    "        splits.append((train_df, test_df))\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n=== FOLD {fold_idx}/{k_folds} ===\")\n",
    "        print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "        \n",
    "        for split_name, split_df in [(\"Train\", train_df), (\"Test\", test_df)]:\n",
    "            print(f\"\\n{split_name} set:\")\n",
    "            print(\"  Corpus distribution:\")\n",
    "            for corpus in target_corpus_props.keys():\n",
    "                count = (split_df['corpus'] == corpus).sum()\n",
    "                prop = count / len(split_df) if len(split_df) > 0 else 0\n",
    "                print(f\"    {corpus}: {count} ({prop:.2%}) [target: {target_corpus_props[corpus]:.2%}]\")\n",
    "            \n",
    "            print(\"  Correction distribution:\")\n",
    "            for corrected_val in [False, True]:\n",
    "                count = (split_df['corrected'] == corrected_val).sum()\n",
    "                prop = count / len(split_df) if len(split_df) > 0 else 0\n",
    "                label = \"left-as-is\" if not corrected_val else \"corrected\"\n",
    "                print(f\"    {label}: {count} ({prop:.2%}) [target: {target_correction_props[corrected_val]:.2%}]\")\n",
    "    \n",
    "    # Drop temporary column\n",
    "    df.drop('strat_key', axis=1, inplace=True)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# ============================================================\n",
    "# SAVE ALIGNMENT FILES\n",
    "# ============================================================\n",
    "\n",
    "def save_alignment_files(df, out_dir, set_name):\n",
    "    \"\"\"\n",
    "    Save alignment input files from DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with 'src' and 'tgt' columns\n",
    "    out_dir : output directory\n",
    "    set_name : name for the file set (e.g., 'train', 'test')\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nCreating alignment files for: {set_name}\")\n",
    "    print(f\"Total sentence pairs: {len(df)}\")\n",
    "    \n",
    "    # File paths\n",
    "    src_path = os.path.join(out_dir, f\"{set_name}_src.txt\")\n",
    "    tgt_path = os.path.join(out_dir, f\"{set_name}_tgt.txt\")\n",
    "    src_tok = os.path.join(out_dir, f\"{set_name}_src.tok\")\n",
    "    tgt_tok = os.path.join(out_dir, f\"{set_name}_tgt.tok\")\n",
    "    src_segm = os.path.join(out_dir, f\"{set_name}_src.segm\")\n",
    "    tgt_segm = os.path.join(out_dir, f\"{set_name}_tgt.segm\")\n",
    "    \n",
    "    with open(src_path, \"w\", encoding=\"utf-8\") as fsrc, \\\n",
    "         open(tgt_path, \"w\", encoding=\"utf-8\") as ftgt, \\\n",
    "         open(src_tok, \"w\", encoding=\"utf-8\") as fsrc_tok, \\\n",
    "         open(tgt_tok, \"w\", encoding=\"utf-8\") as ftgt_tok, \\\n",
    "         open(src_segm, \"w\", encoding=\"utf-8\") as fsrc_seg, \\\n",
    "         open(tgt_segm, \"w\", encoding=\"utf-8\") as ftgt_seg:\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            s = str(row['src']).strip()\n",
    "            t = str(row['tgt']).strip()\n",
    "            \n",
    "            # Write plain text\n",
    "            fsrc.write(s + \"\\n\")\n",
    "            ftgt.write(t + \"\\n\")\n",
    "            \n",
    "            # Tokenized\n",
    "            s_tok = simple_tokenize(s)\n",
    "            t_tok = simple_tokenize(t)\n",
    "            fsrc_tok.write(\" \".join(s_tok) + \"\\n\")\n",
    "            ftgt_tok.write(\" \".join(t_tok) + \"\\n\")\n",
    "            \n",
    "            # Segmented (underscore-separated)\n",
    "            fsrc_seg.write(segment_with_underscores(s) + \"\\n\")\n",
    "            ftgt_seg.write(segment_with_underscores(t) + \"\\n\")\n",
    "    \n",
    "    print(f\"✓ Alignment files created in: {out_dir}\")\n",
    "    print(f\"  - {set_name}_src.txt / {set_name}_tgt.txt (plain)\")\n",
    "    print(f\"  - {set_name}_src.tok / {set_name}_tgt.tok (tokenized)\")\n",
    "    print(f\"  - {set_name}_src.segm / {set_name}_tgt.segm (segmented)\")\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROCESSING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def process_csv_to_alignment(csv_path, out_dir=\"alignment_output\", k_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Main function: Load CSV, apply stratified sampling with sklearn, create alignment files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : path to input CSV file\n",
    "    out_dir : output directory for alignment files\n",
    "    k_folds : number of cross-validation folds\n",
    "    random_state : random seed\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    print(f\"Loading CSV from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Verify required columns\n",
    "    required_cols = ['src', 'tgt', 'corpus', 'corrected']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    print(f\"\\nTotal sentences loaded: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Convert 'corrected' to boolean if needed\n",
    "    if df['corrected'].dtype == 'object':\n",
    "        df['corrected'] = df['corrected'].map({'True': True, 'False': False, True: True, False: False})\n",
    "    \n",
    "    # Perform stratified split with sklearn\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMING STRATIFIED K-FOLD SPLIT (using sklearn)\")\n",
    "    print(\"=\"*60)\n",
    "    splits = stratified_split(df, k_folds=k_folds, random_state=random_state)\n",
    "    \n",
    "    # Save alignment files for each fold\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING ALIGNMENT FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for fold_idx, (train_df, test_df) in enumerate(splits, 1):\n",
    "        fold_dir = os.path.join(out_dir, f\"fold_{fold_idx}\")\n",
    "        \n",
    "        save_alignment_files(train_df, fold_dir, f\"train\")\n",
    "        save_alignment_files(test_df, fold_dir, f\"test\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ ALL PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Output directory: {out_dir}\")\n",
    "    print(f\"Number of folds: {k_folds}\")\n",
    "    print(f\"Method: sklearn.model_selection.StratifiedKFold\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    csv_file = \"all_corpora.csv\"  # Replace with your CSV path\n",
    "    output_directory = \"aligned_sets\"\n",
    "    \n",
    "    # Process CSV and create alignment files\n",
    "    splits = process_csv_to_alignment(\n",
    "        csv_path=csv_file,\n",
    "        out_dir=output_directory,\n",
    "        k_folds=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\nYou can access the splits programmatically:\")\n",
    "    print(\"  splits[0] = (train_df_fold1, test_df_fold1)\")\n",
    "    print(\"  splits[1] = (train_df_fold2, test_df_fold2)\")\n",
    "    print(\"  etc.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
