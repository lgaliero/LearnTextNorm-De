{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5270be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load spaCy with sentencizer\n",
    "def load_spacy(model=\"de_core_news_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(model, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "    except:\n",
    "        nlp = spacy.blank(\"de\")\n",
    "    if \"sentencizer\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    return nlp\n",
    "\n",
    "nlp = load_spacy()\n",
    "nlp.max_length = 2_000_000\n",
    "\n",
    "# Process one corpus file (TXT)\n",
    "def process_corpus_spacy(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return {\"n_sentences\": 0, \"words\": 0, \"unique_tokens\": 0, \"avg_words_per_sentence\": 0}\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "    num_sent = len(sentences)\n",
    "    num_words = len(tokens)\n",
    "    unique_tokens = len(set(tokens))\n",
    "    avg_words_per_sentence = num_words / num_sent if num_sent else 0\n",
    "    \n",
    "    return {\n",
    "        \"n_sentences\": num_sent,\n",
    "        \"words\": num_words,\n",
    "        \"unique_tokens\": unique_tokens,\n",
    "        \"avg_words_per_sentence\": round(avg_words_per_sentence, 2)\n",
    "    }\n",
    "\n",
    "# Process CSV with spaCy row-by-row (much faster than concatenating)\n",
    "def process_csv_stats_spacy_optimized(df_subset):\n",
    "    \"\"\"\n",
    "    Process CSV data with spaCy row-by-row to avoid memory issues.\n",
    "    Each sentence is already split in the CSV, so we process individually.\n",
    "    \"\"\"\n",
    "    total_pairs = len(df_subset)\n",
    "    corrected_pairs = df_subset['corrected'].sum()\n",
    "    left_as_is = total_pairs - corrected_pairs\n",
    "    \n",
    "    # Count sentences (each row = 1 sentence pair = 2 sentences)\n",
    "    total_sentences = total_pairs * 2\n",
    "    corrected_sentences = corrected_pairs * 2\n",
    "    uncorrected_sentences = left_as_is * 2\n",
    "    \n",
    "    all_tokens = []\n",
    "    \n",
    "    # Process src column row-by-row\n",
    "    for idx, text in enumerate(df_subset['src'].fillna('')):\n",
    "        if text and str(text).strip():\n",
    "            try:\n",
    "                doc = nlp(str(text))\n",
    "                tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                all_tokens.extend(tokens)\n",
    "            except Exception as e:\n",
    "                # Skip problematic rows\n",
    "                continue\n",
    "    \n",
    "    # Process tgt column row-by-row\n",
    "    for idx, text in enumerate(df_subset['tgt'].fillna('')):\n",
    "        if text and str(text).strip():\n",
    "            try:\n",
    "                doc = nlp(str(text))\n",
    "                tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                all_tokens.extend(tokens)\n",
    "            except Exception as e:\n",
    "                # Skip problematic rows\n",
    "                continue\n",
    "  \n",
    "    print()  # New line after progress indicators\n",
    "    \n",
    "    num_words = len(all_tokens)\n",
    "    unique_tokens = len(set(all_tokens))\n",
    "    avg_words_per_sentence = num_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    return {\n",
    "        \"n_sentence_pairs\": total_pairs,\n",
    "        \"n_sentences\": total_sentences,\n",
    "        \"words\": num_words,\n",
    "        \"unique_tokens\": unique_tokens,\n",
    "        \"avg_words_per_sentence\": round(avg_words_per_sentence, 2),\n",
    "        \"corrected_pairs\": int(corrected_pairs),\n",
    "        \"left_as_is\": int(left_as_is),\n",
    "        \"corrected_pairs_pct\": f\"{round(corrected_pairs / total_pairs * 100, 2)}%\" if total_pairs else \"0%\",\n",
    "        \"corrected_sentences\": int(corrected_sentences),\n",
    "        \"uncorrected_sentences\": int(uncorrected_sentences),\n",
    "        \"corrected_sentences_pct\": round(corrected_sentences / total_sentences * 100, 2) if total_sentences else 0\n",
    "    }\n",
    "\n",
    "def compute_corpus_stats(source=\"both\", csv_path=\"all_corpora.csv\"):\n",
    "    \"\"\"\n",
    "    Compute statistics on corpus data.\n",
    "    \n",
    "    Args:\n",
    "        source: \"txt\", \"csv\", or \"both\"\n",
    "        csv_path: Path to CSV file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with statistics\n",
    "    \"\"\"\n",
    "    corpus_files = {\n",
    "        \"LEONIDE\": \"LEONIDE_full.txt\",\n",
    "        \"Kolipsi_1_L1\": \"Kolipsi_1_L1_full.txt\",\n",
    "        \"Kolipsi_1_L2\": \"Kolipsi_1_L2_full.txt\",\n",
    "        \"Kolipsi_2\": \"Kolipsi_2_full.txt\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process TXT files\n",
    "    if source in [\"txt\", \"both\"]:\n",
    "        print(\"=== Processing TXT files ===\\n\")\n",
    "        \n",
    "        # Individual corpora\n",
    "        txt_results = []\n",
    "        for name, path in corpus_files.items():\n",
    "            try:\n",
    "                stats = process_corpus_spacy(path)\n",
    "                stats[\"corpus\"] = name\n",
    "                stats[\"source\"] = \"TXT\"\n",
    "                txt_results.append(stats)\n",
    "                results.append(stats)\n",
    "                print(f\"✓ Processed {name}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found: {path}\")\n",
    "            except ValueError as e:\n",
    "                if \"exceeds maximum\" in str(e):\n",
    "                    print(f\"✗ {name}: Text too large, skipping detailed analysis\")\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # Whole TXT corpus - aggregate from individual stats\n",
    "        if txt_results:\n",
    "            all_stats = {\n",
    "                \"corpus\": \"WHOLE_CORPUS\",\n",
    "                \"source\": \"TXT\",\n",
    "                \"n_sentences\": sum(r[\"n_sentences\"] for r in txt_results),\n",
    "                \"words\": sum(r[\"words\"] for r in txt_results),\n",
    "                \"unique_tokens\": sum(r[\"unique_tokens\"] for r in txt_results),\n",
    "                \"avg_words_per_sentence\": 0\n",
    "            }\n",
    "            if all_stats[\"n_sentences\"] > 0:\n",
    "                all_stats[\"avg_words_per_sentence\"] = round(\n",
    "                    all_stats[\"words\"] / all_stats[\"n_sentences\"], 2\n",
    "                )\n",
    "            results.append(all_stats)\n",
    "            print(f\"✓ Aggregated combined TXT corpus stats\")\n",
    "    \n",
    "    # Process CSV file\n",
    "    if source in [\"csv\", \"both\"]:\n",
    "        print(\"\\n=== Processing CSV file ===\\n\")\n",
    "        \n",
    "        try:\n",
    "            df_csv = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "            \n",
    "            # Individual corpora from CSV\n",
    "            corpus_names = sorted(df_csv['corpus'].unique())\n",
    "            \n",
    "            for corpus_name in corpus_names:\n",
    "                df_subset = df_csv[df_csv['corpus'] == corpus_name]\n",
    "                stats = process_csv_stats_spacy_optimized(df_subset)\n",
    "                stats[\"corpus\"] = corpus_name\n",
    "                stats[\"source\"] = \"CSV\"\n",
    "                results.append(stats)\n",
    "            \n",
    "            # Whole CSV corpus\n",
    "            all_csv_stats = process_csv_stats_spacy_optimized(df_csv)\n",
    "            all_csv_stats[\"corpus\"] = \"WHOLE_CORPUS\"\n",
    "            all_csv_stats[\"source\"] = \"CSV\"\n",
    "            results.append(all_csv_stats)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ CSV file not found: {csv_path}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    base_cols = [\"corpus\"]\n",
    "    \n",
    "    if \"n_sentence_pairs\" in df_results.columns:\n",
    "        other_cols = [\"n_sentence_pairs\", \"n_sentences\", \"words\", \"unique_tokens\", \n",
    "                     \"avg_words_per_sentence\", \"corrected_pairs\", \"left_as_is\", \n",
    "                     \"corrected_pairs_pct\"]\n",
    "    else:\n",
    "        other_cols = [\"n_sentences\", \"words\", \"unique_tokens\", \"avg_words_per_sentence\"]\n",
    "    \n",
    "    available_cols = base_cols + [col for col in other_cols if col in df_results.columns]\n",
    "    df_results = df_results[available_cols]\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "def compute_corrected_only_stats(csv_path=\"LearnTextNorm-De/all_corpora.csv\"):\n",
    "    \"\"\"\n",
    "    Compute statistics for corrected pairs only.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with corrected-only statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_csv_full = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "        df_corrected_only = df_csv_full[df_csv_full['corrected'] == True]\n",
    "        \n",
    "        if len(df_corrected_only) == 0:\n",
    "            print(\"No corrected pairs found in the dataset.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        corrected_stats = []\n",
    "        corpus_names = sorted(df_corrected_only['corpus'].unique())\n",
    "        \n",
    "        # Per-subcorpus stats for corrected pairs only\n",
    "        for corpus_name in corpus_names:\n",
    "            df_subset = df_corrected_only[df_corrected_only['corpus'] == corpus_name]\n",
    "            \n",
    "            all_tokens = []\n",
    "            \n",
    "            # Process src\n",
    "            for text in df_subset['src'].fillna(''):\n",
    "                if text and str(text).strip():\n",
    "                    try:\n",
    "                        doc = nlp(str(text))\n",
    "                        tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                        all_tokens.extend(tokens)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # Process tgt\n",
    "            for text in df_subset['tgt'].fillna(''):\n",
    "                if text and str(text).strip():\n",
    "                    try:\n",
    "                        doc = nlp(str(text))\n",
    "                        tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                        all_tokens.extend(tokens)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            num_words = len(all_tokens)\n",
    "            unique_tokens = len(set(all_tokens))\n",
    "            total_sentences = len(df_subset) * 2  # Each pair = 2 sentences\n",
    "            avg_words = num_words / total_sentences if total_sentences else 0\n",
    "            \n",
    "            corrected_stats.append({\n",
    "                'corpus': corpus_name,\n",
    "                'corrected_pairs': len(df_subset),\n",
    "                'words': num_words,\n",
    "                'unique_tokens': unique_tokens,\n",
    "                'avg_words_per_sentence': round(avg_words, 2)\n",
    "            })\n",
    "            print(f\"  ✓ Done\\n\")\n",
    "        \n",
    "        # Whole corpus corrected pairs\n",
    "        print(f\"  Processing ALL corrected pairs ({len(df_corrected_only):,} rows)...\")\n",
    "        all_tokens_corrected = []\n",
    "        \n",
    "        for text in df_corrected_only['src'].fillna(''):\n",
    "            if text and str(text).strip():\n",
    "                try:\n",
    "                    doc = nlp(str(text))\n",
    "                    tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                    all_tokens_corrected.extend(tokens)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        for text in df_corrected_only['tgt'].fillna(''):\n",
    "            if text and str(text).strip():\n",
    "                try:\n",
    "                    doc = nlp(str(text))\n",
    "                    tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                    all_tokens_corrected.extend(tokens)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        num_words_all = len(all_tokens_corrected)\n",
    "        unique_tokens_all = len(set(all_tokens_corrected))\n",
    "        total_sentences_all = len(df_corrected_only) * 2\n",
    "        avg_words_all = num_words_all / total_sentences_all if total_sentences_all else 0\n",
    "        \n",
    "        corrected_stats.append({\n",
    "            'corpus': 'ALL_CORRECTED',\n",
    "            'corrected_pairs': len(df_corrected_only),\n",
    "            'words': num_words_all,\n",
    "            'unique_tokens': unique_tokens_all,\n",
    "            'avg_words_per_sentence': round(avg_words_all, 2)\n",
    "        })\n",
    "        \n",
    "        return pd.DataFrame(corrected_stats)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ CSV file not found: {csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# CONTROL PANEL - CUSTOMIZE YOUR OUTPUT HERE\n",
    "\n",
    "# 1. Choose data source\n",
    "SOURCE = \"csv\"  # Options: \"txt\", \"csv\", or \"both\"\n",
    "\n",
    "# 2. Choose which DataFrames to display (True = show, False = hide)\n",
    "SHOW_MAIN_STATS = True              # Main corpus statistics table\n",
    "SHOW_CORRECTION_BREAKDOWN = True    # Correction stats by subcorpus\n",
    "SHOW_CORRECTION_SUMMARY = True      # Overall correction summary\n",
    "SHOW_CORRECTED_ONLY_STATS = True    # Detailed stats for corrected pairs only\n",
    "\n",
    "\n",
    "# MAIN EXECUTION - THIS ACTUALLY RUNS THE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CORPUS STATISTICS (Source: {SOURCE.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Main Statistics\n",
    "    if SHOW_MAIN_STATS:\n",
    "        df_stats = compute_corpus_stats(source=SOURCE, csv_path=\"all_corpora.csv\")\n",
    "        display(df_stats)\n",
    "\n",
    "    # Additional CSV-specific analyses\n",
    "    if SOURCE in [\"csv\", \"both\"]:\n",
    "        \n",
    "        # 2. Correction Breakdown by Subcorpus\n",
    "        if SHOW_CORRECTION_BREAKDOWN:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"CORRECTION STATISTICS BREAKDOWN\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            try:\n",
    "                df_csv_full = pd.read_csv(\"all_corpora.csv\", encoding=\"utf-8\")\n",
    "                \n",
    "                print(\"\\n--- By Subcorpus ---\")\n",
    "                correction_by_corpus = df_csv_full.groupby('corpus')['corrected'].agg([\n",
    "                    ('total_pairs', 'count'),\n",
    "                    ('corrected_pairs', 'sum'),\n",
    "                    ('left_as_is', lambda x: (~x).sum()),\n",
    "                    ('corrected_pct', lambda x: f\"{round(x.sum() / len(x) * 100, 2)}%\")\n",
    "                ]).reset_index()\n",
    "                \n",
    "                display(correction_by_corpus)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(\"✗ CSV file not found for correction analysis\")\n",
    "        \n",
    "        # 3. Overall Correction Summary\n",
    "        if SHOW_CORRECTION_SUMMARY:\n",
    "            try:\n",
    "                if 'df_csv_full' not in locals():\n",
    "                    df_csv_full = pd.read_csv(\"all_corpora.csv\", encoding=\"utf-8\")\n",
    "                \n",
    "                print(\"\\n--- Whole Corpus ---\")\n",
    "                total_pairs = len(df_csv_full)\n",
    "                corrected_pairs = df_csv_full['corrected'].sum()\n",
    "                left_as_is = total_pairs - corrected_pairs\n",
    "                \n",
    "                overall_stats = pd.DataFrame([{\n",
    "                    'Metric': 'Total Sentence Pairs',\n",
    "                    'Count': total_pairs,\n",
    "                    'Percentage': '100.00%'\n",
    "                }, {\n",
    "                    'Metric': 'Corrected Pairs (True)',\n",
    "                    'Count': int(corrected_pairs),\n",
    "                    'Percentage': f\"{corrected_pairs/total_pairs*100:.2f}%\"\n",
    "                }, {\n",
    "                    'Metric': 'Left-As-Is Pairs (False)',\n",
    "                    'Count': int(left_as_is),\n",
    "                    'Percentage': f\"{left_as_is/total_pairs*100:.2f}%\"\n",
    "                }])\n",
    "                \n",
    "                display(overall_stats)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(\"✗ CSV file not found for correction analysis\")\n",
    "        \n",
    "        # 4. Corrected Pairs Only - Detailed Stats\n",
    "        if SHOW_CORRECTED_ONLY_STATS:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"CORRECTED PAIRS ONLY - DETAILED STATISTICS\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            df_corrected_stats = compute_corrected_only_stats(csv_path=\"all_corpora.csv\")\n",
    "            if not df_corrected_stats.empty:\n",
    "                display(df_corrected_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
