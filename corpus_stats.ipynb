{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5270be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CORPUS STATISTICS (Source: CSV)\n",
      "================================================================================\n",
      "\n",
      "=== Processing CSV file ===\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>n_sentence_pairs</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>avg_words_per_sentence</th>\n",
       "      <th>corrected_pairs</th>\n",
       "      <th>left_as_is</th>\n",
       "      <th>corrected_pairs_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kolipsi_1_L1</td>\n",
       "      <td>5017</td>\n",
       "      <td>10034</td>\n",
       "      <td>137209</td>\n",
       "      <td>6757</td>\n",
       "      <td>13.67</td>\n",
       "      <td>987</td>\n",
       "      <td>4030</td>\n",
       "      <td>19.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kolipsi_1_L2</td>\n",
       "      <td>5081</td>\n",
       "      <td>10162</td>\n",
       "      <td>112658</td>\n",
       "      <td>5640</td>\n",
       "      <td>11.09</td>\n",
       "      <td>1835</td>\n",
       "      <td>3246</td>\n",
       "      <td>36.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kolipsi_2</td>\n",
       "      <td>6085</td>\n",
       "      <td>12170</td>\n",
       "      <td>150133</td>\n",
       "      <td>6128</td>\n",
       "      <td>12.34</td>\n",
       "      <td>2223</td>\n",
       "      <td>3862</td>\n",
       "      <td>36.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEONIDE</td>\n",
       "      <td>4399</td>\n",
       "      <td>8798</td>\n",
       "      <td>118080</td>\n",
       "      <td>7369</td>\n",
       "      <td>13.42</td>\n",
       "      <td>2022</td>\n",
       "      <td>2377</td>\n",
       "      <td>45.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WHOLE_CORPUS</td>\n",
       "      <td>20582</td>\n",
       "      <td>41164</td>\n",
       "      <td>518080</td>\n",
       "      <td>17631</td>\n",
       "      <td>12.59</td>\n",
       "      <td>7067</td>\n",
       "      <td>13515</td>\n",
       "      <td>34.34%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus  n_sentence_pairs  n_sentences   words  unique_tokens  \\\n",
       "0  Kolipsi_1_L1              5017        10034  137209           6757   \n",
       "1  Kolipsi_1_L2              5081        10162  112658           5640   \n",
       "2     Kolipsi_2              6085        12170  150133           6128   \n",
       "3       LEONIDE              4399         8798  118080           7369   \n",
       "4  WHOLE_CORPUS             20582        41164  518080          17631   \n",
       "\n",
       "   avg_words_per_sentence  corrected_pairs  left_as_is corrected_pairs_pct  \n",
       "0                   13.67              987        4030              19.67%  \n",
       "1                   11.09             1835        3246              36.11%  \n",
       "2                   12.34             2223        3862              36.53%  \n",
       "3                   13.42             2022        2377              45.96%  \n",
       "4                   12.59             7067       13515              34.34%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CORRECTION STATISTICS BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "--- By Subcorpus ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>total_pairs</th>\n",
       "      <th>corrected_pairs</th>\n",
       "      <th>left_as_is</th>\n",
       "      <th>corrected_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kolipsi_1_L1</td>\n",
       "      <td>5017</td>\n",
       "      <td>987</td>\n",
       "      <td>4030</td>\n",
       "      <td>19.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kolipsi_1_L2</td>\n",
       "      <td>5081</td>\n",
       "      <td>1835</td>\n",
       "      <td>3246</td>\n",
       "      <td>36.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kolipsi_2</td>\n",
       "      <td>6085</td>\n",
       "      <td>2223</td>\n",
       "      <td>3862</td>\n",
       "      <td>36.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEONIDE</td>\n",
       "      <td>4399</td>\n",
       "      <td>2022</td>\n",
       "      <td>2377</td>\n",
       "      <td>45.96%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus  total_pairs  corrected_pairs  left_as_is corrected_pct\n",
       "0  Kolipsi_1_L1         5017              987        4030        19.67%\n",
       "1  Kolipsi_1_L2         5081             1835        3246        36.11%\n",
       "2     Kolipsi_2         6085             2223        3862        36.53%\n",
       "3       LEONIDE         4399             2022        2377        45.96%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Whole Corpus ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Sentence Pairs</td>\n",
       "      <td>20582</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Corrected Pairs (True)</td>\n",
       "      <td>7067</td>\n",
       "      <td>34.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Left-As-Is Pairs (False)</td>\n",
       "      <td>13515</td>\n",
       "      <td>65.66%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Metric  Count Percentage\n",
       "0      Total Sentence Pairs  20582    100.00%\n",
       "1    Corrected Pairs (True)   7067     34.34%\n",
       "2  Left-As-Is Pairs (False)  13515     65.66%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CORRECTED PAIRS ONLY - DETAILED STATISTICS\n",
      "================================================================================\n",
      "  ✓ Done\n",
      "\n",
      "  ✓ Done\n",
      "\n",
      "  ✓ Done\n",
      "\n",
      "  ✓ Done\n",
      "\n",
      "  Processing ALL corrected pairs (7,067 rows)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>corrected_pairs</th>\n",
       "      <th>words</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>avg_words_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kolipsi_1_L1</td>\n",
       "      <td>987</td>\n",
       "      <td>32873</td>\n",
       "      <td>3457</td>\n",
       "      <td>16.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kolipsi_1_L2</td>\n",
       "      <td>1835</td>\n",
       "      <td>49424</td>\n",
       "      <td>4153</td>\n",
       "      <td>13.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kolipsi_2</td>\n",
       "      <td>2223</td>\n",
       "      <td>65455</td>\n",
       "      <td>4525</td>\n",
       "      <td>14.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEONIDE</td>\n",
       "      <td>2022</td>\n",
       "      <td>62952</td>\n",
       "      <td>5846</td>\n",
       "      <td>15.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALL_CORRECTED</td>\n",
       "      <td>7067</td>\n",
       "      <td>210704</td>\n",
       "      <td>12752</td>\n",
       "      <td>14.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          corpus  corrected_pairs   words  unique_tokens  \\\n",
       "0   Kolipsi_1_L1              987   32873           3457   \n",
       "1   Kolipsi_1_L2             1835   49424           4153   \n",
       "2      Kolipsi_2             2223   65455           4525   \n",
       "3        LEONIDE             2022   62952           5846   \n",
       "4  ALL_CORRECTED             7067  210704          12752   \n",
       "\n",
       "   avg_words_per_sentence  \n",
       "0                   16.65  \n",
       "1                   13.47  \n",
       "2                   14.72  \n",
       "3                   15.57  \n",
       "4                   14.91  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load spaCy with sentencizer\n",
    "def load_spacy(model=\"de_core_news_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(model, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "    except:\n",
    "        nlp = spacy.blank(\"de\")\n",
    "    if \"sentencizer\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    return nlp\n",
    "\n",
    "nlp = load_spacy()\n",
    "nlp.max_length = 2_000_000\n",
    "\n",
    "# Process one corpus file (TXT)\n",
    "def process_corpus_spacy(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return {\"n_sentences\": 0, \"words\": 0, \"unique_tokens\": 0, \"avg_words_per_sentence\": 0}\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "    num_sent = len(sentences)\n",
    "    num_words = len(tokens)\n",
    "    unique_tokens = len(set(tokens))\n",
    "    avg_words_per_sentence = num_words / num_sent if num_sent else 0\n",
    "    \n",
    "    return {\n",
    "        \"n_sentences\": num_sent,\n",
    "        \"words\": num_words,\n",
    "        \"unique_tokens\": unique_tokens,\n",
    "        \"avg_words_per_sentence\": round(avg_words_per_sentence, 2)\n",
    "    }\n",
    "\n",
    "# Process CSV with spaCy row-by-row (much faster than concatenating)\n",
    "def process_csv_stats_spacy_optimized(df_subset):\n",
    "    \"\"\"\n",
    "    Process CSV data with spaCy row-by-row to avoid memory issues.\n",
    "    Each sentence is already split in the CSV, so we process individually.\n",
    "    \"\"\"\n",
    "    total_pairs = len(df_subset)\n",
    "    corrected_pairs = df_subset['corrected'].sum()\n",
    "    left_as_is = total_pairs - corrected_pairs\n",
    "    \n",
    "    # Count sentences (each row = 1 sentence pair = 2 sentences)\n",
    "    total_sentences = total_pairs * 2\n",
    "    corrected_sentences = corrected_pairs * 2\n",
    "    uncorrected_sentences = left_as_is * 2\n",
    "    \n",
    "    all_tokens = []\n",
    "    \n",
    "    # Process src column row-by-row\n",
    "    for idx, text in enumerate(df_subset['src'].fillna('')):\n",
    "        if text and str(text).strip():\n",
    "            try:\n",
    "                doc = nlp(str(text))\n",
    "                tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                all_tokens.extend(tokens)\n",
    "            except Exception as e:\n",
    "                # Skip problematic rows\n",
    "                continue\n",
    "    \n",
    "    # Process tgt column row-by-row\n",
    "    for idx, text in enumerate(df_subset['tgt'].fillna('')):\n",
    "        if text and str(text).strip():\n",
    "            try:\n",
    "                doc = nlp(str(text))\n",
    "                tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                all_tokens.extend(tokens)\n",
    "            except Exception as e:\n",
    "                # Skip problematic rows\n",
    "                continue\n",
    "  \n",
    "    print()  # New line after progress indicators\n",
    "    \n",
    "    num_words = len(all_tokens)\n",
    "    unique_tokens = len(set(all_tokens))\n",
    "    avg_words_per_sentence = num_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    return {\n",
    "        \"n_sentence_pairs\": total_pairs,\n",
    "        \"n_sentences\": total_sentences,\n",
    "        \"words\": num_words,\n",
    "        \"unique_tokens\": unique_tokens,\n",
    "        \"avg_words_per_sentence\": round(avg_words_per_sentence, 2),\n",
    "        \"corrected_pairs\": int(corrected_pairs),\n",
    "        \"left_as_is\": int(left_as_is),\n",
    "        \"corrected_pairs_pct\": f\"{round(corrected_pairs / total_pairs * 100, 2)}%\" if total_pairs else \"0%\",\n",
    "        \"corrected_sentences\": int(corrected_sentences),\n",
    "        \"uncorrected_sentences\": int(uncorrected_sentences),\n",
    "        \"corrected_sentences_pct\": round(corrected_sentences / total_sentences * 100, 2) if total_sentences else 0\n",
    "    }\n",
    "\n",
    "def compute_corpus_stats(source=\"both\", csv_path=\"output/all_corpora.csv\"):\n",
    "    \"\"\"\n",
    "    Compute statistics on corpus data.\n",
    "    \n",
    "    Args:\n",
    "        source: \"txt\", \"csv\", or \"both\"\n",
    "        csv_path: Path to CSV file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with statistics\n",
    "    \"\"\"\n",
    "    corpus_files = {\n",
    "        \"LEONIDE\": \"LEONIDE_full.txt\",\n",
    "        \"Kolipsi_1_L1\": \"Kolipsi_1_L1_full.txt\",\n",
    "        \"Kolipsi_1_L2\": \"Kolipsi_1_L2_full.txt\",\n",
    "        \"Kolipsi_2\": \"Kolipsi_2_full.txt\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process TXT files\n",
    "    if source in [\"txt\", \"both\"]:\n",
    "        print(\"=== Processing TXT files ===\\n\")\n",
    "        \n",
    "        # Individual corpora\n",
    "        txt_results = []\n",
    "        for name, path in corpus_files.items():\n",
    "            try:\n",
    "                stats = process_corpus_spacy(path)\n",
    "                stats[\"corpus\"] = name\n",
    "                stats[\"source\"] = \"TXT\"\n",
    "                txt_results.append(stats)\n",
    "                results.append(stats)\n",
    "                print(f\"✓ Processed {name}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found: {path}\")\n",
    "            except ValueError as e:\n",
    "                if \"exceeds maximum\" in str(e):\n",
    "                    print(f\"✗ {name}: Text too large, skipping detailed analysis\")\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # Whole TXT corpus - aggregate from individual stats\n",
    "        if txt_results:\n",
    "            all_stats = {\n",
    "                \"corpus\": \"WHOLE_CORPUS\",\n",
    "                \"source\": \"TXT\",\n",
    "                \"n_sentences\": sum(r[\"n_sentences\"] for r in txt_results),\n",
    "                \"words\": sum(r[\"words\"] for r in txt_results),\n",
    "                \"unique_tokens\": sum(r[\"unique_tokens\"] for r in txt_results),\n",
    "                \"avg_words_per_sentence\": 0\n",
    "            }\n",
    "            if all_stats[\"n_sentences\"] > 0:\n",
    "                all_stats[\"avg_words_per_sentence\"] = round(\n",
    "                    all_stats[\"words\"] / all_stats[\"n_sentences\"], 2\n",
    "                )\n",
    "            results.append(all_stats)\n",
    "            print(f\"✓ Aggregated combined TXT corpus stats\")\n",
    "    \n",
    "    # Process CSV file\n",
    "    if source in [\"csv\", \"both\"]:\n",
    "        print(\"\\n=== Processing CSV file ===\\n\")\n",
    "        \n",
    "        try:\n",
    "            df_csv = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "            \n",
    "            # Individual corpora from CSV\n",
    "            corpus_names = sorted(df_csv['corpus'].unique())\n",
    "            \n",
    "            for corpus_name in corpus_names:\n",
    "                df_subset = df_csv[df_csv['corpus'] == corpus_name]\n",
    "                stats = process_csv_stats_spacy_optimized(df_subset)\n",
    "                stats[\"corpus\"] = corpus_name\n",
    "                stats[\"source\"] = \"CSV\"\n",
    "                results.append(stats)\n",
    "            \n",
    "            # Whole CSV corpus\n",
    "            all_csv_stats = process_csv_stats_spacy_optimized(df_csv)\n",
    "            all_csv_stats[\"corpus\"] = \"WHOLE_CORPUS\"\n",
    "            all_csv_stats[\"source\"] = \"CSV\"\n",
    "            results.append(all_csv_stats)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ CSV file not found: {csv_path}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    base_cols = [\"corpus\"]\n",
    "    \n",
    "    if \"n_sentence_pairs\" in df_results.columns:\n",
    "        other_cols = [\"n_sentence_pairs\", \"n_sentences\", \"words\", \"unique_tokens\", \n",
    "                     \"avg_words_per_sentence\", \"corrected_pairs\", \"left_as_is\", \n",
    "                     \"corrected_pairs_pct\"]\n",
    "    else:\n",
    "        other_cols = [\"n_sentences\", \"words\", \"unique_tokens\", \"avg_words_per_sentence\"]\n",
    "    \n",
    "    available_cols = base_cols + [col for col in other_cols if col in df_results.columns]\n",
    "    df_results = df_results[available_cols]\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "def compute_corrected_only_stats(csv_path=\"LearnTextNorm-Deoutput/all_corpora.csv\"):\n",
    "    \"\"\"\n",
    "    Compute statistics for corrected pairs only.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with corrected-only statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_csv_full = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "        df_corrected_only = df_csv_full[df_csv_full['corrected'] == True]\n",
    "        \n",
    "        if len(df_corrected_only) == 0:\n",
    "            print(\"No corrected pairs found in the dataset.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        corrected_stats = []\n",
    "        corpus_names = sorted(df_corrected_only['corpus'].unique())\n",
    "        \n",
    "        # Per-subcorpus stats for corrected pairs only\n",
    "        for corpus_name in corpus_names:\n",
    "            df_subset = df_corrected_only[df_corrected_only['corpus'] == corpus_name]\n",
    "            \n",
    "            all_tokens = []\n",
    "            \n",
    "            # Process src\n",
    "            for text in df_subset['src'].fillna(''):\n",
    "                if text and str(text).strip():\n",
    "                    try:\n",
    "                        doc = nlp(str(text))\n",
    "                        tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                        all_tokens.extend(tokens)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # Process tgt\n",
    "            for text in df_subset['tgt'].fillna(''):\n",
    "                if text and str(text).strip():\n",
    "                    try:\n",
    "                        doc = nlp(str(text))\n",
    "                        tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                        all_tokens.extend(tokens)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            num_words = len(all_tokens)\n",
    "            unique_tokens = len(set(all_tokens))\n",
    "            total_sentences = len(df_subset) * 2  # Each pair = 2 sentences\n",
    "            avg_words = num_words / total_sentences if total_sentences else 0\n",
    "            \n",
    "            corrected_stats.append({\n",
    "                'corpus': corpus_name,\n",
    "                'corrected_pairs': len(df_subset),\n",
    "                'words': num_words,\n",
    "                'unique_tokens': unique_tokens,\n",
    "                'avg_words_per_sentence': round(avg_words, 2)\n",
    "            })\n",
    "            print(f\"  ✓ Done\\n\")\n",
    "        \n",
    "        # Whole corpus corrected pairs\n",
    "        print(f\"  Processing ALL corrected pairs ({len(df_corrected_only):,} rows)...\")\n",
    "        all_tokens_corrected = []\n",
    "        \n",
    "        for text in df_corrected_only['src'].fillna(''):\n",
    "            if text and str(text).strip():\n",
    "                try:\n",
    "                    doc = nlp(str(text))\n",
    "                    tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                    all_tokens_corrected.extend(tokens)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        for text in df_corrected_only['tgt'].fillna(''):\n",
    "            if text and str(text).strip():\n",
    "                try:\n",
    "                    doc = nlp(str(text))\n",
    "                    tokens = [tok.text for tok in doc if tok.is_alpha]\n",
    "                    all_tokens_corrected.extend(tokens)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        num_words_all = len(all_tokens_corrected)\n",
    "        unique_tokens_all = len(set(all_tokens_corrected))\n",
    "        total_sentences_all = len(df_corrected_only) * 2\n",
    "        avg_words_all = num_words_all / total_sentences_all if total_sentences_all else 0\n",
    "        \n",
    "        corrected_stats.append({\n",
    "            'corpus': 'ALL_CORRECTED',\n",
    "            'corrected_pairs': len(df_corrected_only),\n",
    "            'words': num_words_all,\n",
    "            'unique_tokens': unique_tokens_all,\n",
    "            'avg_words_per_sentence': round(avg_words_all, 2)\n",
    "        })\n",
    "        \n",
    "        return pd.DataFrame(corrected_stats)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ CSV file not found: {csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# CONTROL PANEL - CUSTOMIZE YOUR OUTPUT HERE\n",
    "\n",
    "# 1. Choose data source\n",
    "SOURCE = \"csv\"  # Options: \"txt\", \"csv\", or \"both\"\n",
    "\n",
    "# 2. Choose which DataFrames to display (True = show, False = hide)\n",
    "SHOW_MAIN_STATS = True              # Main corpus statistics table\n",
    "SHOW_CORRECTION_BREAKDOWN = True    # Correction stats by subcorpus\n",
    "SHOW_CORRECTION_SUMMARY = True      # Overall correction summary\n",
    "SHOW_CORRECTED_ONLY_STATS = True    # Detailed stats for corrected pairs only\n",
    "\n",
    "\n",
    "# MAIN EXECUTION - THIS ACTUALLY RUNS THE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CORPUS STATISTICS (Source: {SOURCE.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Main Statistics\n",
    "    if SHOW_MAIN_STATS:\n",
    "        df_stats = compute_corpus_stats(source=SOURCE, csv_path=\"output/all_corpora.csv\")\n",
    "        display(df_stats)\n",
    "\n",
    "    # Additional CSV-specific analyses\n",
    "    if SOURCE in [\"csv\", \"both\"]:\n",
    "        \n",
    "        # 2. Correction Breakdown by Subcorpus\n",
    "        if SHOW_CORRECTION_BREAKDOWN:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"CORRECTION STATISTICS BREAKDOWN\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            try:\n",
    "                df_csv_full = pd.read_csv(\"output/all_corpora.csv\", encoding=\"utf-8\")\n",
    "                \n",
    "                print(\"\\n--- By Subcorpus ---\")\n",
    "                correction_by_corpus = df_csv_full.groupby('corpus')['corrected'].agg([\n",
    "                    ('total_pairs', 'count'),\n",
    "                    ('corrected_pairs', 'sum'),\n",
    "                    ('left_as_is', lambda x: (~x).sum()),\n",
    "                    ('corrected_pct', lambda x: f\"{round(x.sum() / len(x) * 100, 2)}%\")\n",
    "                ]).reset_index()\n",
    "                \n",
    "                display(correction_by_corpus)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(\"✗ CSV file not found for correction analysis\")\n",
    "        \n",
    "        # 3. Overall Correction Summary\n",
    "        if SHOW_CORRECTION_SUMMARY:\n",
    "            try:\n",
    "                if 'df_csv_full' not in locals():\n",
    "                    df_csv_full = pd.read_csv(\"output/all_corpora.csv\", encoding=\"utf-8\")\n",
    "                \n",
    "                print(\"\\n--- Whole Corpus ---\")\n",
    "                total_pairs = len(df_csv_full)\n",
    "                corrected_pairs = df_csv_full['corrected'].sum()\n",
    "                left_as_is = total_pairs - corrected_pairs\n",
    "                \n",
    "                overall_stats = pd.DataFrame([{\n",
    "                    'Metric': 'Total Sentence Pairs',\n",
    "                    'Count': total_pairs,\n",
    "                    'Percentage': '100.00%'\n",
    "                }, {\n",
    "                    'Metric': 'Corrected Pairs (True)',\n",
    "                    'Count': int(corrected_pairs),\n",
    "                    'Percentage': f\"{corrected_pairs/total_pairs*100:.2f}%\"\n",
    "                }, {\n",
    "                    'Metric': 'Left-As-Is Pairs (False)',\n",
    "                    'Count': int(left_as_is),\n",
    "                    'Percentage': f\"{left_as_is/total_pairs*100:.2f}%\"\n",
    "                }])\n",
    "                \n",
    "                display(overall_stats)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(\"✗ CSV file not found for correction analysis\")\n",
    "        \n",
    "        # 4. Corrected Pairs Only - Detailed Stats\n",
    "        if SHOW_CORRECTED_ONLY_STATS:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"CORRECTED PAIRS ONLY - DETAILED STATISTICS\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            df_corrected_stats = compute_corrected_only_stats(csv_path=\"output/all_corpora.csv\")\n",
    "            if not df_corrected_stats.empty:\n",
    "                display(df_corrected_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
